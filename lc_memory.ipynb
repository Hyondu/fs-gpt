{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation buffer memory: keeps every token in the past\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    # return_messages=True\n",
    ")\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation buffer window memory: keeps past conversation window of length k\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "  return_messages=True,\n",
    "  k=4,\n",
    ")\n",
    "\n",
    "def add_messages(input, output):\n",
    "  memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_messages(1, 1)\n",
    "add_messages(2, 2)\n",
    "add_messages(3, 3)\n",
    "add_messages(4, 4)\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_messages(5, 5)\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation summary memory\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=chat)\n",
    "\n",
    "def get_history():\n",
    "  return memory.load_memory_variables({})\n",
    "\n",
    "add_messages(\"Hi I'm Son, I live in South Korea\", \"Whoa, that is so cool!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_messages(\"South Korea is so beautiful.\", \"I wish I could go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation summary buffer memory: mix of summary for old conversations and buffer memory of recent ones\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  return_messages=True,\n",
    "  llm=chat,\n",
    "  max_token_limit=150, # max number of tokens before the buffer memory gets summarized\n",
    ")\n",
    "\n",
    "add_messages(\"Hi I'm Son, I live in South Korea\", \"Whoa, that is so cool!\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_messages(\"South Korea is so beautiful.\", \"I wish I could go!\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_messages(\"How far is South Korea from Argentine?\", \"I don't know! Maybe super far!!\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_messages(\"How far is Brazil from Argentine?\", \"I don't know! Maybe super near!!\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation Knowledge Graph memory: it builds a knowledge graph of entities from the conversation\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\")\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "  llm=chat,\n",
    "  return_messages=True,\n",
    ")\n",
    "\n",
    "add_messages(\"Hi I'm Son, I live in South Korea\", \"Who is Son?\")\n",
    "add_messages(\"Son is your friend.\", \"Okay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({\"input\": \"Who is Son?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_messages(\"Son likes Kimchi.\", \"What does Son like?\")\n",
    "add_messages(\"Son likes Kimchi.\", \"Okay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({\"input\": \"What does Son like?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory on LLMChain\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=chat,\n",
    "  max_token_limit=120,\n",
    "  memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "  You are a helpful AI talking to a human.\n",
    "\n",
    "  {chat_history}\n",
    "  Human: {question}\n",
    "  You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm=chat,\n",
    "  memory=memory,\n",
    "  prompt=PromptTemplate.from_template(template),\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Son.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"I live in Seoul.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat based memory\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=chat,\n",
    "  max_token_limit=120,\n",
    "  memory_key=\"chat_history\",\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful AI talking to a human.\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "  (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm=chat,\n",
    "  memory=memory,\n",
    "  prompt=prompt,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Son.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL based memory\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "  return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | chat\n",
    "\n",
    "def invoke_chain(question):\n",
    "  result = chain.invoke({\"question\": question})\n",
    "  memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/fs-gpt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\" Hello Son, it's nice to meet you. How can I assist you today? If you have any questions or need help with something specific, feel free to ask.\"\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is Son.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\" Your name is Son, as you mentioned earlier in our conversation. Is there anything particular you'd like to know or discuss related to that name or any other topic? I'm here to help answer any questions you might have.\"\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
